#input模块:
#文件类型:
input{
  file{
    #path属性接受的参数是一个数组，其含义是标明需要读取的文件位置
    path => ["pathA","pathB"]
    #表示多就去path路径下查看是够有新的文件产生。默认是15秒检查一次。
    discover_interval => 15
    #排除那些文件，也就是不去读取那些文件
    exclude => ["fileName1","fileNmae2"]
    #被监听的文件多久没更新后断开连接不在监听，默认是一个小时。
    close_older => 3600
    #在每次检查文件列 表的时候， 如果一个文件的最后 修改时间 超过这个值， 就忽略这个文件。 默认一天。
    ignore_older => 86400
    #logstash 每隔多 久检查一次被监听文件状态（ 是否有更新） ， 默认是 1 秒。
    stat_interval => 1
    #sincedb记录数据上一次的读取位置的一个index
    sincedb_path => "$HOME/. sincedb"
    #logstash 从什么 位置开始读取文件数据， 默认是结束位置 也可以设置为：beginning 从头开始
    start_position => "beginning"
    #注意：这里需要提醒大家的是，如果你需要每次都从同开始读取文件的话，关设置start_position => beginning是没有用的，你可以选择sincedb_path 定义为 /dev/null
  }
}

#filter模块
filter{
  #grok插件有非常强大的功能，他能匹配一切数据，但是他的性能和对资源的损耗同样让人诟病
  grok{
    #match属性，作用是从message字段中吧时间给抠出来，并且赋值给另个一个字段logdate。
    #1.所有文本数据都是在Logstash的message字段中的，我们要在过滤器里操作的数据就是message。
    #2.grok插件是一个十分耗费资源的插件，这也是为什么我只打算讲解一个TIMESTAMP_ISO8601正则表达式的原因。
    #3.grok有超级多的预装正则表达式，这里是没办法完全搞定的，也许你可以从这个大神的文章中找到你需要的表达式
    #http://blog.csdn.net/liukuan73/article/details/52318243
    #4.不建议使用它，因为他完全可以用别的插件代替，当然，对于时间这个属性来说，grok是非常便利的。
    match => ["message","%{TIMESTAMP_ISO8601:logdate}"]
  }
  #mutate插件用来处理数据的格式的，可处理时间格式，或把一个字符串变为数字类型。可以设置的转换类型包括： "integer"， "float" 和 "string"。

  mutate {
    #接收一个数组，其形式为value，type
    #注意，数据在转型的时候要合法。
    convert => ["request_time", "float", "costTime", "integer"]
  }

  #ruby插件可以使用任何的ruby语法，无论是逻辑判断，条件语句，循环语句，还是对字符串的操作，对EVENT对象的操作，都是极其得心应手的。
  ruby {
    #ruby插件有两个属性，init和code
    #init属性是用来初始化字段的，可以在这里初始化一个字段，这个字段只是在ruby{}作用域里面生效。
    #这里初始化了一个名为field的hash字段。可以在下面的code属性里面使用。
    init => [field={}]
    #code属性使用两个冒号进行标识，所有ruby语法都可以在里面进行。
    #1.把message字段里面的值拿到，并且对值进行分割按照“|”。这样分割出来的是一个数组（ruby的字符创处理）。
    #2.循环数组判断其值是否是我需要的数据（ruby条件语法、循环结构）
    #3.把需要的字段添加进入EVEVT对象。
    #4.选取一个值，进行MD5加密
    #event对象就是Logstash对象，可以在ruby插件的code属性里面操作他，可以添加属性字段，可以删除，可以修改，同样可以进行树脂运算。
    #进行MD5加密的时候，需要引入对应的包。
    #5.最后把冗余的message字段去除。
    code => "
        array=event。get('message').split('|')
        array.each do |value|
            if value.include? 'MD5_VALUE'
                then
                    require 'digest/md5'
                    md5=Digest::MD5.hexdigest(value)
                    event.set('md5',md5)
            end
            if value.include? 'DEFAULT_VALUE'
                then
                    event.set('value',value)
            end
        end
          remove_field=>"message"
    "
  }

  #date插件需要和grok插件剥离出来的值logdate配合使用（当然也许你不是用grok去做）。
  date{
    #对于老数据来说格式化非常重要，应为你需要修改@timestamp字段的值，如果你不修改，你保存进ES的时间就是系统但前时间（+0时区）
    #当你格式化以后，就可以通过target属性来指定到@timestamp，这样你的数据的时间就会是准确的，这对以你以后图表的建设来说万分重要。
    #最后，logdate这个字段，已经没有任何价值了，所以我们顺手可以吧这个字段从event对象中移除。
    match=>["logdate","dd/MMM/yyyy:HH:mm:ss Z"]
    target=>"@timestamp"
    remove_field => 'logdate'
    #还需要强调的是，@timestamp字段的值，你是不可以随便修改的，最好就按照你数据的某一个时间点来使用，
    #如果是日志，就使用grok把时间抠出来，如果是数据库，就指定一个字段的值来格式化，比如说："timeat", "%{TIMESTAMP_ISO8601:logdate}"
    #timeat就是我的数据库的一个关于时间的字段。
    #如果没有这个字段的话，千万不要试着去修改它。
  }
  #json插件，现在的日志信息，基本都是由固定的样式组成的，可以使用json插件对其进行解析，并且得到每个字段对应的值。
  filter{
    #source指定你的哪个值是json数据。
    json {
      source => "value"
    }
    #注意：如果你的json数据是多层的，那么解析出来的数据在多层结里是一个数组，你可以使用ruby语法对他进行操作，最终把所有数据都装换为平级的。

  }
}

#output模块
#output模块集成了大量的输出插件,可以输出到指定文件,也可输出到指定的网络端口,可以输出数据到ES

output{
  elasticsearch{
    hosts=>["172.132.12.3:9200"]
    #es要执行的动作 index, delete, create, update
    #index:将logstash.时间索引到一个文档
    #delete:根据id删除一个document(这个动作需要一个id)
    #create:建立一个索引document，如果id存在 动作失败.
    #update:根据id更新一个document，有一种特殊情况可以upsert--如果document不是已经存在的情况更新document
    action=>"index"
    #事件要被写进的索引，可是动态的用%{foo}语句
    index=>"indextemplate-logstash"
    #事件要被写入的document type，一般要将相似事件写入同一type，可用%{}引用事件type，默认type=log
    document_type=>"%{@type}"
    #为索引提供document id，对重写elasticsearch中相同id词目很有用
    document_id=>"ignore"
    #一个默认的es mapping 模板将启用（除非设置为false 用自己的template）
    manage_template=>true
    #有效的filepath设置自己的template文件路径，不设置就用已有的
    template=>"/opt/logstash-conf/es-template.json"
    #在es内部模板的名字
    template_name=>"es-template.json"
    template_overwrite=>true
  }

}











